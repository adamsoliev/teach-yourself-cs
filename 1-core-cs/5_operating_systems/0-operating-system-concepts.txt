// Official website supporting Operating System Concepts
https://www.os-book.com/OS10/
// Official source code
https://github.com/greggagne/osc10e

// Programming Challenge Solutions
https://github.com/forestLoop/Learning-EI338
https://github.com/rafi007akhtar/oslab
https://github.com/OmerBaddour/Operating_Systems_Code

-----------------------------------------------------------------------------------

-----------------------------------------
Description                     Chapters
-----------------------------------------
Overview                        1-2 
Process management              3-5
Process synchronization         6-8
Memory management               9-10
Storage management              11-12              
File systems                    13-15
Security and protection         16-17
Advanced topics                 18-19
Cases studies                   20-21
-----------------------------------------

Notes to students
    1) Solve practice exercies at the end of each chapter
    2) Read through the study guide (https://www.os-book.com/OS10/study-guide/Study-Guide.pdf)

------------------------- PART 1 -------------------------
------------ Overview -------------

1. Introduction 
2. Operating-System Structures 

------------------------- PART 2 -------------------------
------------- Process Management  --------------

3. Processes 
4. Threads and Concurrency 
5. CPU Scheduling 

------------------------- PART 3 -------------------------
------------- Process Synchronization  --------------

6. Synchronization Tools 
7. Synchronization Examples 
8. Deadlocks 

------------------------- PART 4 -------------------------
----- Memory Management  ----

9. Main Memory 
10. Virtual Memory 

------------------------- PART 5 -------------------------
----- Storage Management  ----

11. Mass-Storage Structure
12. I/O Systems 

------------------------- PART 6 -------------------------
----- File System  ----

13. File-System Interface
14. File-System Implementation
15. File-System Internals

------------------------- PART 7 -------------------------
----- Security Protection  ----

16. Security
17. Protection

------------------------- PART 8 -------------------------
-------------------- Advanced Topics ---------------------

18. Virtual Machines
19. Networks and Distributed Systems

------------------------- PART 9 -------------------------
---------------------- Case Studies ----------------------

20. The Linux System
21. Window 10

-----------------------------------------------------------------------------------
NVS - Nonvolatile storage
NVM - Nonvolatile electrical storage


-----------------------------------------
1. Introduction
-----------------------------------------

In order to understand the role of OS:
    * Understand hardware resources: CPU, memory, I/O devices, and storage
    * Understand OS pieces and input/output/functions of each
    * Data Structures used in OSs

Hardware Resources
    * CPU ---- system bus ---- memory
               |  |  |  |
            device controllers 
                   | 
                devices
        - OS has a device driver for each device controller

    * Three key aspects of the system
        1) Interrupts
            * Basic implementation
                - device controller raises an interrupt by asserting a singal to CPU
                - CPU catches the interrupt and dispatches it to interrupt handler (via interrupt address table)
                - the handler clears the interrupt by servicing the device

            * Advanced implementation 
                - CPU has two interrupt request lines (nonmaskable and maskable). 
                Nonmaskable is for unrecoverable memory errors. Maskable is for 
                interrupts that can be deferred during critical processing. 
                - Since computers have more devices than entries in interrupt address table,
                each entry points to the head of a list of interrupt handlers.
                - Interrupt mechanism also implements a system of interrupt priority levels, enabling 
                to differenciate low-priority vs high-priority interrupts and defer appropriately.

        2) Storage structure
            * Registers 
            * Cache
            * Main memory
            * Nonvolatile memory (e.g., disk, firmware)
            [1] Storage systems are organized in a hierarchy according to storage cost/capacity, 
            access/transfer time and volatility

        3) I/O structure
            * Since system-bus interrupt-based I/O produces high overhead, the modern systems use direct
            memory access (DMA), which enables a device controller to transfer an entire block of 
            data directly to/from the device and main memory, with no intervention from CPU. Only
            one interrupt is generated per block, to tell the driver that the operation was successful.  
            
            * Some high-end systems use switch rather than bus architecture, enabling multiple 
            components to talk to each other concurrently, rather than competing for cycles on 
            a shared bus. 

    * Computer-System Architecture
        1) Single-Processor Systems
        2) Multiprocessor Systems
        3) Clustered Systems

Operating System 

    * OS consists of:
        - kernel                    - always running
        - middleware frameworks     - ease app development and provide features
        - system programs           - aid in managing the system while it is running

    * OS enables multiprogramming (multiple processes) and multitasking (switching among them)
    * OS can run either in dual-mode (user vs kernel) and multimode (user vs ... vs kernel)
    * OS implements a timer, disabling a user program to get stuck in an infinite loop or fail 
    to call syscall and never return control to the OS
    * OS provides Resource Management
        - Process Management
            • Creating and deleting both user and system processes
            • Scheduling processes and threads on the CPUs
            • Suspending and resuming processes
            • Providing mechanisms for process synchronization
            • Providing mechanisms for process communication

        - Memory Management
            • Keeping track of which parts of memory are currently being used and
            which process is using them
            • Allocating and deallocating memory space as needed
            • Deciding which processes (or parts of processes) and data to move into
            and out of memory
            
        - File-System Management
            • Creating and deleting files
            • Creating and deleting directories to organize files
            • Supporting primitives for manipulating files and directories
            • Mapping files onto mass storage
            • Backing up files on stable (nonvolatile) storage media

        - Mass-Storage Management
            The operating system is responsible for the following activities in 
            connection with 'secondary storage' management:
            • Mounting and unmounting
            • Free-space management
            • Storage allocation
            • Disk scheduling
            • Partitioning
            • Protection

            Because secondary storage is used frequently and extensively, it must be used
            efficiently. The entire speed of operation of a computer may hinge on the speeds
            of the secondary storage subsystem and the algorithms that manipulate that
            subsystem.

        - Cache Management
            • Careful selection of the cache size and of a replacement policy
            can result in greatly increased performance

            • In a hierarchical storage structure, the same data may appear in different
            levels of the storage system. Since the various CPUs can all execute in parallel, 
            we must make sure that an update to the data in one cache is immediately 
            reflected in all other caches where that data resides (cache coherency).

        - I/O System Management
            • A memory-management component that includes buffering, caching, and spooling
            • Ageneral device-driver interface
            • Drivers for specific hardware devices
    * OS provides Security and Protection
    * Virtualization 
        - Single set of hardware hosting multiple virtual machines (different OSs) managed by virtual machine manager
    * Distributed Systems

Data Structures
    * Kernel Data Structures
        - Lists, Stacks, and Queues
        - Trees
        - Hash Functions and Maps
        - Bitmaps

Free and Open-Source Operating Systems
    * GNU/Linux
    * BSD UNIX
    * Solaris

-----------------------------------------
2. Operating-System Structures 
-----------------------------------------
What services an operating system provides                      | Users view
How they are provided and how they are debugged                 | Programmers view
What the various methodologies are for designing such systems   |
How operating systems are created                               | OS designers view
How a computer starts its operating system.                     |

User's view
    * User interface
        - GUI
        - Command-Line Interface (CLI)
            * gets and executes the next user-specified command
            * either contains the code to execute the commands or implements most commands through system programs

    * Program execution
    * I/O operations
    * File-system manipulation
    * Communications 
        - Implemented via shared memory
        - Implemented via message passing

    * Error detections
        - Detect and correct errors in CPU, memory hardware, I/O devices, user program

    * Resource allocation
    * Logging 
    * Protection and security
        - Protect all system resources and defend external I/O devices

Programmer's view
    * System calls provide an interface to the services discussed above. APIs, RTEs (run-time envs) 
    often wrap the system calls to provide the above services

        - Types of system calls
            • Process control
                ◦ create process, terminate process
                ◦ load, execute
                ◦ get process attributes, set process attributes
                ◦ wait event, signal event
                ◦ allocate and free memory
            • File management
                ◦ create file, delete file
                ◦ open, close
                ◦ read, write, reposition
                ◦ get file attributes, set file attributes
            • Device management
                ◦ request device, release device
                ◦ read, write, reposition
                ◦ get device attributes, set device attributes
                ◦ logically attach or detach devices
            • Information maintenance
                ◦ get time or date, set time or date
                ◦ get system data, set system data
                ◦ get process, file, or device attributes
                ◦ set process, file, or device attributes
            • Communications
                ◦ create, delete communication connection
                ◦ send, receive messages
                ◦ transfer status information
                ◦ attach or detach remote devices
            • Protection
                ◦ get file permissions
                ◦ set file permissions

    * System services (utilities)
        - File management
        - Status information
        - File modification (e.g., text editors)
        - Programming-language support (e.g., compilers, assemblers, debuggers, interpreters)
        - Program loading and execution (absolute, relocatable and overlay loaders; linkage editors)
        - Communications
        - Background services (network daemons, process schedulers, system error monitoring)
        - Others (web browsers, word processors and text formatters, spreadsheets, database systems, 
          compilers, plotting and statistical-analysis packages, and games)
    
    * Linkers and Loaders
        - Src program => [compiler] => obj file => [linker] => exe file  => [loader] => program in memory
                                                       |                        |
                                                  other obj files         dynamically linkered
                                                                               libraries

OS Designer's view
    * Goals => policies => mechanisms
        - Define goals and specifications (user goals and system goals) 
        - Those goals determine the OS's policies and the OS implements those policies through mechanisms
            * Separate policy from mechanism
            * Mechanisms determine how to do something; policies determine what will be done
            * The separation of policy and mechanism is important for flexibility
            * A general mechanism flexible enough to work across a range of policies is preferable
    
    * Implementation
        - Now, most are written in higher-level languages such as C or C++, with small amounts 
        of the system written in assembly language.

        - As is true in other systems, major performance improvements in operating
        systems are more likely to be the result of better data structures and algorithms
        than of excellent assembly-language code.

        - Only a small amount of the code is critical to high performance;
        the interrupt handlers, I/O manager, memory manager, and CPU scheduler are
        probably the most critical routines.

    * Operating-System Structure
        - A common approach is to partition the task into small components, or modules, rather
        than have one single system. Each of these modules should be a well-defined
        portion of the system, with carefully defined interfaces and functions.

        - Monolithic (UNIX and Window) vs Layered vs Microkernel (Darwin and QNX) Structure

        - The best current methodology for operating-system design involves
        using loadable kernel modules (LKMs). Here, the kernel has a set of core
        components and can link in additional services via modules, either at boot time
        or during run time. (common modern UNIX, such as Linux, macOS, and Solaris, as well as Windows)

    * Operating-System Debugging
        - The performance of an operating system can be monitored using either counters or tracing
            * Counter: (per-process) ps, top        | (system-wide) vmstat, netstat, iostat
            * Tracing: (per-process) strace, gdb    | (system-wide) perf, tcpdump

            * BCC (BPF Compiler Collection) (https://github.com/iovisor/bcc). What makes BCC 
            especially powerful is that its tools can be used on live production systems that 
            are running critical applications without causing harm to the system.

-----------------------------------------
3. Processes 
-----------------------------------------
One of the most important aspects of an operating system is how it
schedules threads onto available processing cores. Several choices for
designing CPU schedulers are available to programmers.

Process
    * Layout in memory
        ---------   max
        | stack | 
        ---------
        |   ↓   |
        |       |
        |   ↑   |
        ---------
        | heap  |
        ---------
        | data  |
        ---------
        | text  |
        ---------   0
    * States: new, running, waiting, ready, terminated
    * Process Control Block (PCB) - representation of a process in the OS 
        - Process state             - above
        - PC                        - address of next instruction 
        - CPU registers             - general-purpose, etc and condition-code info 
        - CPU-scheduling info       - process priority, pointers to scheduling queues, etc
        - Memory-management info    - value of base/limit registers, page/segment tables
        - Accounting info           - amount of CPU and real time used, time limits, account numbers, job/process numbers
        - I/O status info           - list of I/O devices allocated to the process, list of open files, etc
        - Thread info                
    * Process scheduling
        - # of processes vs # of cores; I/O-bound vs CPU-bound processes
        - Scheduling Queues
            a new process ---> ready queue --------> CPU --->
                                  |                  |
                                  |                  |
                                   -- wait queues* <-
                * there are various type of wait queues
                                           
        - CPU Scheduling - CPU scheduler selecting a process from a ready queue

        - Context Switch
            * Save the state (PCB) of cur process for later restoration; load the state of next process 
    * Operations on Processes
        - Process Creation: process tree, pid
        - Process Termination: cascading termination, zombie and orphan processes
    * Interprocess Communication (IPC)
        - Process can be either independent and cooperating (it can affect or be 
        affected by the other processes executing in the system); the latter requires IPC
        - Types of IPC
            * shared memory - can be implemented using either unbounded or bounded buffer that 
            producer/consumer processes share
            * message passing
                • Direct (A <-> B) or indirect communication (A <-> mailbox <-> B)
                • Synchronous (wait until send/receive finish) or asynchronous communication
                • Automatic or explicit buffering - messages exchanged by communicating processes 
                reside in a temporary queue, which could be implemented
                    - Zero capacity         | synch 
                    - Bounded capacity      | limited asynch ( anynch until the buffer reaches capacity)
                    - Unbounded capacity    | asynch

        - Two common forms of client–server communication are sockets and
        remote procedure calls (RPCs). Sockets allow two processes on different
        machines to communicate over a network. RPCs abstract the concept of
        function (procedure) calls in such a way that a function can be invoked on
        another process that may reside on a separate computer.


-----------------------------------------
4. Threads and Concurrency 
-----------------------------------------

Overview
Multicore programming
Multithreading Models
Thread Libraries
Implicit Threading
Threading Issues
Operating System Examples

* Thread creation and managemenet

               Process
 -----------------------------------
|       code    data    files       |
 -----------------------------------
| registers | registers | registers |
| stack     | stack     | stack     |   
| PC        | PC        | PC        |   
 -----------------------------------
|    \      |     \     |     \     |
|    /      |     /     |     /     | → thread
|    \      |     \     |     \     |
 -----------------------------------

 * Benefits of a thread
    - Responsiveness
    - Resource sharing
    - Economy
    - Scalability

* Programming challenges of multicore systems 
    - Dividing and balancing the work 
    - Data splitting
    - Data dependency
    - Testing and debugging
    
* Types of parallelism
    - Data parallelism - distributing subsets of data across multiple cores
    - Task parallelism - distributing tasks (threads) across multiple cores 

* Multithreading Models
    - User vs kernel threads
    - Establishing a relationship between user vs kernel threads
        * many-to-one model (many user threads map to one kernel thread)
        * one-to-one model (Linux/Windows)
        * many-to-many model 
    - Asynchronous (parent/child run concurrently) vs synchronous threading (parent waits until children terminate)

* Implicit threading - creation of threads by compilers and run-time libraries
    - Thread pools
        * Create a certain # of threads and put them into the pool. Each incoming task is submitted to the pool.
        If a thread is available, the task is served. Otherwise, it is queued till a thread becomes available. 
    - Fork join
        * Synchronious version of thread pools where a certain # of threads is created by a library to 
        complete a can-be-parallel task.
    - OpenMP
        * a set of compiler directives as well as an API for programs written in
        C, C++, or FORTRAN that provides support for parallel programming in shared-memory environments.
        OpenMP identifies parallel regions as blocks of code that may run in parallel. It creates as many 
        threads as there are processing cores in the system.
    - Grand Central Dispatch
        * GCD schedules tasks for run-time execution by placing them on either serial or concurrent dispatch
        queue. When it removes a task from a queue, it assigns the task to an available thread from a pool 
        of threads that it manages. Serial queue is local for each process while concurrent is global.   
    - Intel Thread Building Blocks

* Threading Issues
    - fork() and exec()
    - Signal Handling
        * Deliver the signal to the thread to which the signal applies.
        * Deliver the signal to every thread in the process.
        * Deliver the signal to certain threads in the process.
        * Assign a specific thread to receive all signals for the process.
    - Thread Cancellation
        * Asynchronous cancellation
        * Deferred cancellation 
    - Thread-Local Storage
    - Scheduler Activations

* Operating System Examples

-----------------------------------------
5. CPU Scheduling 
-----------------------------------------
CPU scheduling deals with the problem of deciding which of the processes in
the ready queue is to be allocated the CPU ’s core.

* Basic Concepts
    - CPU–I/O Burst Cycle
    - CPU Scheduler
        * Selects a process from the processes in memory that are ready to execute 
        and allocates the CPU to that process.
    - Preemptive and Nonpreemptive Scheduling
        * Preemptive scheduling algorithms allow a running process to be interrupted by the operating system 
        in order to give resources to another process while nonpreemptive ones do not.
    - Dispatcher
        * Switches context from one process to another and from kernel to user mode
        * Jumps to the proper location in the user program to resume that program

* Scheduling Criteria
    - CPU utilization
    - Throughput
        * # of processes completed per unit of time 
    - Turnaround (ready queue + executing + I/O), Waiting (ready queue) and Response (1st response) time

* Scheduling Algorithms
    - First-Come, First-Served Scheduling
        * Nonpreemptive, simple to implement, very inefficient
        
    - Shortest-Job-First Scheduling
        * Either preemptive or nonpreemptive, optimal, can't be implemented at CPU scheduling level
         as there is no way to know the length of the next CPU burst. One way is to predict it.

    - Round-Robin Scheduling
        * allocates the CPU to each process for a time quantum. If the process does not relinquish 
        the CPU before its time quantum expires, the process is preempted, and another process is scheduled
        to run for a time quantum. 

    - Priority Scheduling
        * assigns each process a priority, and the CPU is allocated to the process with 
        the highest priority. Processes with the same priority can be scheduled in FCFS order or using RR scheduling. 

    - Multilevel Queue Scheduling
        * partitions processes into several separate queues arranged by priority, 
        and the scheduler executes the processes in the highest-priority queue. 
        Different scheduling algorithms may be used in each queue.
        
    - Multilevel Feedback Queue Scheduling
        * similar to multilevel queues, except that a process may migrate between different queues.
        * most general but also most complex algorithm

* Thread Scheduling
    - Contention Scope
        * user-level (scheduled by a thread library that uses process-contention scope (PCS) scheme)
        * kernel-level (scheduled by the OS that uses system-contention scope (SCS) scheme)

* Multi-Processor Scheduling
    - Approaches to Multiple-Processor Scheduling
        * asymmetric multiprocessing - one master for system activities and other processors for user code
        * symmetric multiprocessing (SMP) - where each processor is self-scheduling
            1. All threads may be in a common ready queue.
            2. Each processor may have its own private queue of threads, which is supported by all major OSs

    - Multicore Processors
        * memory stall - when a CPU accesses memory, it spends time waiting for the data to become available
            - multithreaded processing solves this; in it, two (or more) hardware threads are assigned to each core
                1. coarse-grained multithreading - doesn't switch until a long-latency event, such as memory stall 
                2. fine-grained multithreading - switches between threads at a much finer level of granularity
        * since the resources of the physical core must be shared among its hardware threads, a multithreaded, 
        multicore processor actually requires two different levels of scheduling: at software and hardware level

    - Load Balancing (on SMP systems)
        * equalizes loads between CPU cores, although migrating threads between cores to balance loads may invalidate
        cache contents and therefore may increase memory access times.

    - Processor Affinity
        * due to high cost of cache invalidation and repopulation, threads aren't moved to completely new processors (affinity)
        * soft affinity (no guarantee) vs hard affinity (guarantee)

    - Heterogeneous Multiprocessing

* Real-Time CPU Scheduling
    - soft real-time systems and hard real-time systems
    - Minimizing Latency
        * Event latency
            1. Interrupt latency
            2. Dispatch latency
    - Priority-Based Scheduling
    - Rate-Monotonic Scheduling
        * schedules periodic tasks using a static priority policy with preemption (priorities don't change)

    - Earliest-Deadline-First Scheduling
        * assigns priorities dynamically according to deadline (priorities can change)

    - Proportional Share Scheduling
        * operate by allocating T shares among all applications
        * must work with an admission-control policy to guarantee that an app receives its allocated shares of time

    - POSIX Real-Time Scheduling

-----------------------------------------
6. Synchronization Tools 
-----------------------------------------

* The Critical-Section Problem
    - to design a protocol that the processes can use to synchronize their activity so as to
    cooperatively share data.
        * A solution to the critical-section problem must satisfy the following three requirements:
            - Mutual exclusion  - ensures that only one process at a time is active in its critical section
            - Progress          - ensures that programs will cooperatively determine what process will 
                                next enter its critical section
            - Bounded waiting   - limits how much time a program will wait before it can enter its critical section
        * Two approaches to handle critical sections in OSs
            - preemptive kernels 
            - nonpreemptive kernels

* Sofware solutions - don't work well on modern computer architectures
    - Peterson’s Solution

* Hardware Support for Synchronization
    - Memory Barriers
        * Memory modes
            - Strongly ordered  - memory modification in one CPU is immediately visible to other CPUs
            - Weakly ordered    - it isn't the case
            * memory fences (load/store fences) are used to ensure all load/store operations, coming before
            the fence, are completed before initializing load/store operations, coming after the fence
    - Hardware Instructions
        * test_and_set()
            ---------------------------------------------------------------
            while (true) {
                // Try to set the lock to true
                if (test_and_set(&lock)) {
                    // Lock is already set, so wait until it is available
                    continue;
                }
                // Lock is now set, so enter critical section
                // Critical section code goes here
                // Exit critical section
                lock = false;
            }
            ---------------------------------------------------------------

        * compare_and_swap()
            ---------------------------------------------------------------
            while (true) {
                // Try to set the lock to true
                bool expected_value = false;
                if (!compare_and_swap(&lock, expected_value, true)) {
                    // Lock is already set, so wait until it is available
                    continue;
                }
                // Lock is now set, so enter critical section
                // Critical section code goes here
                // Exit critical section
                lock = false;
            }
            ---------------------------------------------------------------

        * load-link/store-condtional()
        - all are executed atomically as one uninterrupted unit
    - Atomic Variables
        * special variables, operations on whom are implemented using compare_and_swap()

* Mutex Locks
    - Uses a mutex lock (and its corresponding acquire()/release()) to solve the critical section problem
    - Disadvantage of this is busy waiting - if a lock is taken, other processes must loop continiously in the call to acquire() 
    - The simplest of synchronization tools

* Semaphores
    - Int variable only accessed through standard atomic wait() and signal() (similar to acquire()/release())
    - Types
        * Counting semaphores   - can be used to control access to a resource consisting of X number of instances
        * Binary semaphores     - similar to mutex lock
    - Also suffers from busy waiting - solution is let it suspend itself if semaphore is unavailable and wake up when available

* Monitor
    - A monitor type is an ADT that includes a set of programmer-defined
    operations that are provided with mutual exclusion within the monitor.
    - The monitor construct ensures that only one process at a time is active
    within the monitor

    Monitor in C
    -----------------------------------------------------------------------
    // Structure to represent the monitor
    typedef struct {
        int shared_data; // Shared data protected by the monitor
        pthread_mutex_t lock; // Mutex used to implement the monitor lock
    } Monitor;
    -----------------------------------------------------------------------

* Liveness
    - Deadlock
    - Priority Inversion

* Evaluation

-----------------------------------------
7. Synchronization Examples 
-----------------------------------------

* Classic Problems of Synchronization
    - The Bounded-Buffer Problem
        * The bounded-buffer problem involves ensuring that the producers do not 
        try to add data to the buffer when it is full, and that the consumers do 
        not try to remove data from the buffer when it is empty.

    - The Readers – Writers Problem
        * The goal of the readers–writers problem is to ensure that multiple threads 
        can read from the shared resource at the same time, but that only one thread 
        can write to the resource at a time.

    - The Dining-Philosophers Problem
        * In the Dining Philosophers problem, there are five philosophers sitting 
        around a table, each with a plate of food in front of them. In order to eat, 
        a philosopher needs to pick up both the fork to their left and the fork to their 
        right. However, there is only one fork between each pair of philosophers, so 
        if one philosopher picks up the left fork, they may need to wait for the philosopher 
        to their right to release the right fork before they can eat.

* Synchronization within the Kernel
    - Synchronization in Windows is done using different types of dispatcher objects below
        * Mutexes: a synchronization object that allows only a single thread 
        to access a shared resource at a time.

        * Semaphores: a synchronization object that allows multiple threads 
        to access a shared resource, but limits the number of threads that can access the 
        resource simultaneously.

        * Events: a synchronization object that allows threads to be signaled 
        or unsignaled, and can be used to synchronize the execution of threads.

        * Timer Queues: a system-defined object that allows a thread to specify 
        a function to be called at a specified time in the future.

        * Timer Objects: a system-defined object that allows a thread to 
        specify a function to be called at a specified interval.

    - Synchronization in Linux is done using mechanisms below
        * Mutexes

        * Semaphores

        * Spinlocks: a type of mutual exclusion lock

        * Reader-writer locks: allows multiple threads to read a shared resource 
        concurrently, but only allows a single thread to write to the resource at a time.

        * Condition variables: a synchronization object that allows a thread to wait for a 
        specific condition to be met before proceeding. They are often used in conjunction 
        with mutexes to synchronize the execution of threads.

        * Atomic operations: used to perform operations on shared variables in a way that 
        ensures that the operation is completed without interference from other threads or processes.

* POSIX Synchronization
    - Mutexes
    - Semaphores
    - Condition variables
    - Read-write locks

* Synchronization in Java
    - Java monitors
    - Reentrant locks
    - Semaphores
    - Condition variables

* Alternative Approaches
    - Transactional Memory
        * A higher level of abstraction for synchronization than traditional synchronization mechanisms
        * It allows threads to execute transactions, which are sequences of memory accesses that 
        are treated as atomic units - that is, other threads are prevented from accessing them

    - OpenMP
        * Provides barriers, locks, atomic operations, critical sections for synchronizing 
        the execution of threads

    - Functional Programming Languages
        * they do not maintain state - their variables are immutable
        * Essentially, most of the problems addressed in this chapter are nonexistent in functional languages.

 
-----------------------------------------
8. Deadlocks 
-----------------------------------------
* Livelock vs deadlock
    - State: Deadlock occurs when two or more processes are blocked and unable to proceed, whereas in a livelock, 
    the processes are still active and running, but not making progress.

    - Resources: In a deadlock, processes are blocked waiting for a resource that is held by another process, 
    whereas in a livelock, the processes are actively trying to acquire a shared resource, but 
    none of them are able to do so.

    - Detection: Deadlocks are typically easier to detect than livelocks because they leave clear signs, 
    such as blocked or suspended processes, whereas livelocks are harder to detect because the processes 
    are still active and running.

    - Resolution: Deadlocks can be resolved by killing one of the processes involved in the deadlock, or 
    by rolling back the actions of one of the processes. Livelocks can be resolved by adding a random delay 
    in the loop that acquires the resource, or by using a hierarchy of the resources.

* Deadlock Characterization
    - Necessary Conditions
        * Mutual exclusion
        * Hold and wait
        * No preemption
        * Circular wait

    - Resource-Allocation Graph
        * A cycle, A->B->C->D->A, in which each process is requesting a resource that is held 
        by another process in the cycle, is a clear indication of a deadlock.

        * It's also possible to detect deadlock if the graph has no cycle but multiple disconnected subgraphs, 
        where a process in a subgraph is holding a resource needed by a process in another subgraph.

* Methods for Handling Deadlocks
    - Deadlock prevention - ensure that at least one of the necessary conditions can't hold
        * Only the circular-wait condition presents an opportunity for a practical solution 
        by invalidating one of the necessary conditions
    - Deadlock avoidance - ensure preventative measures (e.g., using RAG) are taken
        * Resource-Allocation-Graph Algorithm - single instance of each resource type
        * Banker’s Algorithm - multiple instance of each resource type
    - Deadlock detection and recovery - ensure to detect deadlocks and recover from them
        * Wait-for Graph Algorithm - single instance of each resource type
        * Detection Algorithm - multiple instance of each resource type
    - Timeout 






