// Book's website
http://csapp.cs.cmu.edu/3e/students.html

// Labs
http://csapp.cs.cmu.edu/3e/labs.html

// Unofficial solutions
https://dreamanddead.github.io/CSAPP-3e-Solutions/

// C/C++ | compiler | assembler playground
https://godbolt.org/

// Notes and Labs by Jonny Kong
https://github.com/JonnyKong/CMU-15-213-Intro-to-Computer-Systems


-----------------------------------------------------------------------------------
1. A Tour of Computer Systems

----------------------------------------------------------
------------------------- PART 1 -------------------------
------------ Program Structure and Execution -------------
----------------------------------------------------------

2. Representing and Manipulating Information
3. Machine-Level Representation of Programs 
4. Processor Architecture 
5. Optimizing Program Performance 
6. The Memory Hierarchy 

----------------------------------------------------------
------------------------- PART 2 -------------------------
------------- Running Programs on a System  --------------
----------------------------------------------------------

7. Linking 
8. Exceptional Control Flow 
9. Virtual Memory 

----------------------------------------------------------
------------------------- PART 3 -------------------------
----- Interaction and Communication between Programs  ----
----------------------------------------------------------

10. System-Level I/O
11. Network Programming 
12. Concurrent Programming 
-----------------------------------------------------------------------------------


----------------------------------------------------------
1. A Tour of Computer Systems
----------------------------------------------------------

  a) Information Is Bits + Context
    chars => ASCII => bytes

  b) Programs Are Translated by Other Programs into Different Forms
      - Compilation process 
          preprocessor (.c => .i) - fills in '#' directives 
          compiler     (.i => .s) - translates 'C' to assembly language 
          assembler    (.s => .o) - translates assembly to machine-level instructions
          linker       (.o => .o) - links relocatable obj files into one executable obj file

  c) Processors Read and Interpret Instructions Stored in Memory
      - Hardware organization of a system
          buses       - transfers fixed-size (4 | 8) chunks of bytes known as 'words'
          I/O devices - connected to I/O bus by either controller or adapter (modeled as files)
          main memory - a collection of dynamic random access memory (DRAM) chips
          CPU         - contains PC (program counter), register file (a
                        collection of word-sized registers), ALU
                        (arithmetic/logical unit) and bus interface      

  d) Caches Matter
      - L1, L2 (and L3) - increasingly large and decreasingly fast. Have the
        same performance characteristics as registers but considerable larger
      - Implemented with static random access memory (SRAM).

  e) Storage Devices Form a Hierarchy
      - L0 (registers) <=> L1 (SRAM) <=> L2 (SRAM) <=> L3 (SRAM) <=> L4 (DRAM)
        <=> L5 (local disks) <=> L6 (distributed file systems, web servers)
      - Each level serves as a cache for the level below.

  f) The Operating System Manages the Hardware
      |---------------------------------------|
      |         Application programs          |
      |---------------------------------------|
      |                 OS                    |
      |---------------------------------------|
      | Processor | Main memory | I/O devices |
      |---------------------------------------|
      |           |             |             | 
      |           |             |--- Files ---|
      |           |                           |
      |           |------ Virtual memory -----|
      |                                       |
      |--------------- Processes -------------|

      - process is the OS's abstraction for running a program. each process has its
        own context (state) and is managed by the OS's kernel. It can consist of
        multiple execution units (threads).

      - virtual memory (VM)
          kernel virtual memory   ↑
          user stack              ↑
          shared libraries        ↑ increasing address index 
          run-time heap           ↑
          program code and data   ↑

  g) Systems Communicate with Other Systems Using Networks

      - Important Themes
          * Concurrency - general concept of having multiple, simultaneous activities
          * Parallelism - concurrency's use to make a system run faster
              - thread-level parallelism 
                  * multiple threads per single process 

              - instruction-level parallelism
                  * executing multiple instructions at the same time

              - single-instruciton, multiple-data (SIMD) parallelism
                  * single instruction causing mutliple operations to be
                    performated at the same time

          * Abstractions in computer systems
              - on processor side
                  * instruction set architecture - one instruction set for all
                    types of processor implementations

              - on operating system side
                  * files, virtual memory, processes

----------------------------------------------------------
2. Representing and Manipulating Information
----------------------------------------------------------

The basic definitions of the encodings, derivation such
properties as the range of representable numbers, their bit-level
representations, and the properties of the arithmetic operations.

  a) The three most important representations of numbers. 
      - Unsigned (0 and up)
      - Two's-complement (most common way to represent signed ints (- & +))
      - Floating point (real numbers)

  b) Information Storage
      - binary (machine) - hexadecimal (easy to communicate) - decimal (humans)
          ---------------------------------------------------------
          Hex digit           0    1    2    3    4    5    6    7
          Decimal value       0    1    2    3    4    5    6    7
          Binary value     0000 0001 0010 0011 0100 0101 0110 0111

          Hex digit           8    9    A    B    C    D    E    F
          Decimal value       8    9   10   11   12   13   14   15
          Binary value     1000 1001 1010 1011 1100 1101 1110 1111
          ---------------------------------------------------------

      - word size (w-bit) => virtual address space (0 - ([2^w] - 1)); for example,
      32-bit word size limits the vas to 4GB, while 64-bit leads to a vas of 16
      exabytes. 

          -------------------------------------------------
              C declaration                     Bytes
          ------------------------------    ---------------
          Signed          Unsigned          32-bit  64-bit
          -------------------------------------------------
          [signed] char   unsigned char       1       1
          short           unsigned short      2       2
          int             unsigned            4       4
          long            unsigned long       4       8
          int32_t         uint32_t            4       4
          int64_t         uint64_t            8       8
          char*                               4       8
          float                               4       4
          double                              8       8
          -------------------------------------------------
          * Using fixed-size integer types (intN_t where N = 8 16 32 64) is the
          best way for programmers to have close control over data
          representations across different machines and compilers.

      - addressing and byte-ordering
          * a multi-byte object is stored as a contiguous sequence of bytes, with the
          address of the object given by the smallest address of the bytes used.

          * byte-ordering

              variable x of type int at address 0x100 has a hex value of 0x01234567
              -------------------------------------------------
              address           0x100   0x101   0x102   0x103
              -------------------------------------------------
              big endian        01      23      45      67        // most significant byte comes first
              little endian     67      45      23      01        // least significant byte comes first
              -------------------------------------------------

      - Boolean Algebra
          -----------------------------------------------
            Operation     Math      CS      Sets
          -----------------------------------------------
            NOT           ¬         ~       complement
            AND           ∧         &       intersection 
            OR            ∨         |       union
            NOR           ⊕         ^
          -----------------------------------------------

            0110      0110      0110
          & 1100    | 1100    ^ 1100    ~ 1100
            ----      ----      ----      ----
            0100      1110      1010      0011

          One useful application of bit vectors is to represent finite sets. We can
          encode any subsetA ⊆ {0, 1, . . . , w − 1} with a bit vector [aw−1, . . . ,
          a1, a0], where ai = 1 if and only if i ∈ A. For example, bit vector a =
          [0110 1001] encodes the set A = {0, 3, 5, 6}, while encodes [0101 0101] {0,
          2, 4, 6}. 

      - Shift operations
          ----------------------------------------------------------------
          operation             Value 1      Value 2
          ----------------------------------------------------------------
          Argument x            [0110 0011]  [1001 0101]
          x << 4                [0011 0000]  [0101 0000]
          x >> 4 (logical)      [0000 0110]  [0000 1001]  *zero extend
          x >> 4 (arithmetic)   [0000 0110]  [1111 1001]  *sign-bit extend
          ----------------------------------------------------------------

  c) Integer Representation
      B2U, B2T (binary to unsigned & two's complement)
      U2B, U2T (unsigned to binary & two's complement)
      T2B, T2U (two's complement to binary & unsigned)
      
      - Unsigned encodings 
          * Maps w-bit binary into {0, ..., ((2^w) - 1)} = [0, ..., UMaxw]
          [1011] => 1*2^3 + 0*2^2 + 1*2^1 + 1*2^0 = 11
          [1111] => 1*2^3 + 1*2^2 + 1*2^1 + 1*2^0 = 15

      - Two's complement encodings (signed)
          * Maps w-bit binary into {-2^(w-1), ..., 2^(w-1) - 1} = [TMinw, ..., TMaxw]
          [1011] => -1*2^3 + 0*2^2 + 1*2^1 + 1*2^0 = -5
          [1111] => -1*2^3 + 1*2^2 + 1*2^1 + 1*2^0 = -1 
          * most significant bit has negative weight.

          Important numbers
          ------------------------------------------------------------------------------------------
                                                          Word size w
                          --------------------------------------------------------------------------
          Value             8             16              32                    64
          ------------------------------------------------------------------------------------------
          UMaxw           0xFF          0xFFFF        0xFFFFFFFF        0xFFFFFFFFFFFFFFFF
                          255           65,535        4,294,967,295     18,446,744,073,709,551,615
          TMinw           0x80          0x8000        0x80000000        0x8000000000000000
                          −128          −32,768       −2,147,483,648    −9,223,372,036,854,775,808
          TMaxw           0x7F          0x7FFF        0x7FFFFFFF        0x7FFFFFFFFFFFFFFF
                          127           32,767        2,147,483,647     9,223,372,036,854,775,807
          −1              0xFF          0xFFFF        0xFFFFFFFF        0xFFFFFFFFFFFFFFFF
           0              0x00          0x0000        0x00000000        0x0000000000000000
          ------------------------------------------------------------------------------------------

      - Signed <=> Unsigned (casting)
          * the effect of casting is to keep the bit values identical but change
          how these bits are interpreted.

      - Expanding the bit representation of a number
          * for unsigned, add necessary # of zeros to the left
          * for signed, add necessary # of sign-bits to the left

          * One point worth making is that the relative order of conversion from
          one data size to another and between unsigned and signed can affect the
          behavior of a program. 

      - Truncating Numbers
          Going from w-bit to k-bit means we drop the high-order (w - k) bits. In
          decimal terms, this means

          * Unsigned
          d_new = d_old % (2^k)

          * Signed 
          d_new = d_old % (2^k) & convert sign-bit's weight from pos to neg 

      * One way to avoid int bugs is to never use unsigned numbers except in
      packing word with flags, addresses in systems programming and
      implementing math packages.

  d) Integer Arithmetic
      - Unsigned
          * addition:         normal case vs overflow (truncate the result)
              - overflowed iff s < x | y, where s = x + y
              - if overflow: x + y - 2^w
          * negation:         x = 0 ? x : 2^w - x
          * multiplication:   normal vs overflow (truncate the result)
              - if overflow: (x*y) % 2^w

      - Two's complement 
          * addition:         negative overflow vs normal vs positive overflow
              - positively overflowed iff x > 0 & y > 0 but s <= 0 
              - negatively overflowed iff x < 0 & y < 0 but s >= 0
              - if positive overflow: x + y - 2^w
              - if negative overflow: x + y + 2^w
          * negation:         x = MIN ? MIN : -x
          * multiplication:   negative overflow vs normal vs positive overflow
              - if overflow: (x*y) % 2^w & sign-bit's weight from pos to neg 

      - Integer division always rounds toward zero. That is, it should round down
      a positive result and round up a negative one. 

  e) Floating Point
      Value       (-1)^s * M * 2^E
      Binary      [s] [exp] [frac]

      ---------------------------------------------------------
      Part             Meaning
      ---------         ---------------------------------------
        s               sign bit (1 - neg; 0 - pos)
      ---------------------------------------------------------
        exp [E]         E = e - Bias (normal) | E = 1 - Bias (denormal)
                          e       k-bit unsigned number 
                          Bias    2^(k-1) - 1
      ---------------------------------------------------------
        frac [M]        fractional value 
              -----------------------------------------------------------------------------
              Exp         Case            Binary representation             Value
              ------      ------------    -------------------------         ---------------
                          Normal          M = 1.f(n-1)...f(1)f(0)           1<=M<2
              -----------------------------------------------------------------------------
              0s          Denormalized    M = 0.f(n-1)...f(1)f(0)           0<=M<1
              -----------------------------------------------------------------------------
              1s          Special*        M = all zeroes || nonzero         infinity || NaN
              -----------------------------------------------------------------------------
              * When exp is 'Normal' (not all 0s or 1s), the 1st bit of M is assumed to be 1, 
              which adds value (2^0 * 1) to the M. Hence, in the normal case, M is at least 1. 
              The rest comes from the fractional bits.
      
      -----------------------------------------------------------------------
      C types                       s     exp                 frac
      -----------------------------------------------------------------------
      float (single-precision)      1     8 (-126/+127)       23      32-bit
      double (double-precision)     1     11 (-1022/+1023)    52      64-bit
      -----------------------------------------------------------------------


    Rounding
      ----------------------------------------------------------------
        Mode                        1.40   1.60   1.50   2.50   –1.50
      ----------------------------------------------------------------
        Round-to-even (default)       1      2      2      2      –2
        Round-toward-zero             1      1      1      2      –1
        Round-down                    1      1      1      2      –2
        Round-up                      2      2      2      3      –1
      ----------------------------------------------------------------

----------------------------------------------------------
3. Machine-Level Representation of Programs 
----------------------------------------------------------

----------- Hardware background -----------
At the lowest level (almost), it runs due to the actions of transistors.
Transistors in digital logic circuits are used mostly as switches. You want the
transistors to be either all the way OFF (no current flow) or all the way ON
(lots of current flow). And you can use the output of one transistor to control
the input of other transistors, so you can construct a complicated circuit by
wiring them up in certain ways. 

Using just a few transistors, you can build very simple logic circuits to
implement binary digital logic (called "Boolean" logic after the guy who
invented it). It turns out that you can build any digital logic function with a
combination of just a few simple circuit types, such as AND/OR/NOT (you can
actually do everything with just one, but no one really does that). By
combining those simple circuits, more complicated circuits can be made (such as
an adder, a multiplexer, etc.). You can use those circuits to make even more
complicated ones. Like a CPU. 
// source
https://www.reddit.com/r/explainlikeimfive/comments/17faj8/eli5_how_computer_chips_work/
-------------------------------------------

  C => assembly code (object code) => machine code (executable code)

  Being able to understand assembly code and how it relates to the
  original C code is a key step in understanding how computers execute programs.


  a) Representatioand manipulation of data 
      Two abstractions are importnat for machine-level programming. 
          1) the format and behavior of a machine-level program is defined by the
          'instruction set architecture' (ISA) defining:
              * the processor state
                  a) program counter (address of the next instruction to be executed)
                  b) integer register file (16 locations storing 64-bit values)
                  c) conditional code registers
                  d) a set of vector registers (each can hold one or multiple int or
                  float values) 

              * the format of instructions (x86-64)
                  a) Instructions can range in length from 1 to 15 bytes (commonly used
                  ones and ones with more operands require a smaller # of bytes).

                  b) The instruction format is designed in such a way that from a given
                  starting position, there is a unique decoding of the bytes into
                  machine instructions.

                  c) Intel vs ATT formats (this file follows ATT)
                      * The size designation suffixes: 'push'/'mov' instead of 'pushq'/'movq'
                      * The ‘%’ character in front of register names: using 'rbx' instead of '%rbx'
                      * Describing locations in memory: 'QWORD PTR [rbx]' rather than '(%rbx)'
                      * Instructions with multiple operands list them in the reverse order. 

                  d) Lines beginning with ‘.’ are directives to guide the assembler and linker

                  e) Data formats
                      ----------------------------------------------------------------------------------
                      C declaration       Intel data type       Assembly-code suffix        Size (bytes)
                      ----------------------------------------------------------------------------------
                      char                Byte                          b                       1
                      short               Word                          w                       2
                      int                 Double word                   l                       4
                      long                Quad word                     q                       8
                      char*               Quad word                     q                       8
                      float               Single precision              s                       4
                      double              Double precision              l                       8
                      ----------------------------------------------------------------------------------

                  f) Accessing information
                      Integer registers (stores 64-bit values)
                          ------------------------------------------------------------------
                          63                31            15        7
                          ------------------------------------------------------------------
                          %rax              %eax          %ax       %al       Return value
                          %rbx              %ebx          %bx       %bl       Callee saved 
                          %rcx              %ecx          %cx       %cl       4th argument 
                          %rdx              %edx          %dx       %dl       3rd argument 
                          %rsi              %esi          %si       %sil      2nd argument 
                          %rdi              %edi          %di       %dil      1st argument 
                          %rbp              %ebp          %bp       %bpl      Callee saved 
                          %rsp              %esp          %sp       %spl      Stack pointer 
                          %r8               %r8d          %r8w      %r8b      5th argument
                          %r9               %r9d          %r9w      %r9b      6th argument
                          %r10              %r10d         %r10w     %r10b     Callee saved 
                          %r11              %r11d         %r11w     %r11b     Callee saved 
                          %r12              %r12d         %r12w     %r12b     Callee saved 
                          %r13              %r13d         %r13w     %r13b     Callee saved 
                          %r14              %r14d         %r14w     %r14b     Callee saved 
                          %r15              %r15d         %r15w     %r15b     Callee saved 
                          ------------------------------------------------------------------
                          * Instructions that generate 1- or 2-byte values, leave remaining bytes unchanged 
                          * Instructions that generate 4-byte values, set remaining bytes to zero 

                      Operands 
                          Instruction [src value operand] [dest loc operand]

                          Forms supported by x86-64: immediate, register and memory reference
                          --------------------------------------------------------------------------------------
                          Type                Form              Operand value               Name
                          --------------------------------------------------------------------------------------
                          Immediate           $Imm              Imm                         Immediate
                          Register            ra                R[ra]                       Register
                          Memory              Imm               M[Imm]                      Absolute
                          Memory              (ra)              M[R[ra]]                    Indirect
                          Memory              Imm(rb)           M[Imm + R[rb]]              Base + displacement
                          Memory              (rb,ri)           M[R[rb]+ R[ri]]             Indexed
                          Memory              Imm(rb,ri)        M[Imm + R[rb]+ R[ri]]       Indexed
                          Memory              (,ri,s)           M[R[ri] . s]                Scaled indexed
                          Memory              Imm(,ri,s)        M[Imm + R[ri] . s]          Scaled indexed
                          Memory              (rb,ri,s)         M[R[rb]+ R[ri] . s]         Scaled indexed
                          Memory (general)    Imm(rb,ri,s)      M[Imm + R[rb]+ R[ri] . s]   Scaled indexed
                          --------------------------------------------------------------------------------------
                          * a, b, i - subscripts
                          * $Imm - constant values
                          * ra - arbitrary register a; ra - content of register is value; (ra) - content of register is address
                          * Mb[Addr] - a reference to b-byte value stored in memory starting at Addr (subscript b is dropped for simplicity)
                          * M[x] - different addressing modes
                          * Imm(rb,ri,s) - an immediate offset Imm, a base register rb, an index register ri , and a scale factor s, 
                          where s must be 1, 2, 4, or 8

                      Data Movement Instructions
                          * src value       - immediate, stored in a register or stored in memory
                          * dest location   - register or memory address 

                          - mov (movb, movw, movl, movq)
                          - movz (movzbw, movzbl, movzwl, movzbq, movzwq)           - move zero-extended 'x' to 'y'
                          - movs (movsbw, movsbl, movswl, movsbq, movswq, movslq)   - move sign-extended 'x' to 'y'
                          - pushq & popq (onto and from the program stack)
                          * movz & movs are used to copy a smaller src value to a larger dest loc
                          ** stack grows downward; top element has the lowest address

                      Arithmetic and Logical Instructions
                          ---------------------------------------------------------------
                          Instruction       Effect            Description
                          ---------------------------------------------------------------
                          leaq S,D          D ← &S            Load effective address    // used to generate pointers & compactly describe arith ops
                          inc D             D ← D+1           Increment
                          dec D             D ← D−1           Decrement
                          neg D             D ← -D            Negate
                          not D             D ← ~D            Complement
                          add S,D           D ← D + S         Add
                          sub S,D           D ← D − S         Subtract
                          imul S,D          D ← D ∗ S         Multiply
                          xor S,D           D ← D ^ S         Exclusive-or
                          or S,D            D ← D | S         Or
                          and S,D           D ← D & S         And
                          sal k,D           D ← D<<k          Left shift
                          shl k,D           D ← D<<k          Left shift (same as sal)
                          sar k,D           D ← D>>A k        Arithmetic right shift
                          shr k,D           D ← D>>L k        Logical right shift
                          ---------------------------------------------------------------
                          * shift amount k can be specified with either immediate value or 1-byte register %cl 
                          * if %cl, lower m bits of %cl determine k (2^m = w, where w is size of data value) 

              * the effect each of these instructions will have on the state. 

          2) the memory addresses used by a machine-level program are 'virtual
          addresses'

  b) Implementation of control
  
      There are two low-level mechanisms for conditinal behavior:
          1) test and alter control flow 
          2) test and alter data flow
          * alteration of either flow is implemented using jump instruction

      Implementation of conditional operations (test above)
          Condition code registers:
              CF - carry flag; most recent operation yielded unsigned overflow
              ZF - zero flag; most recent operation yielded zero
              SF - sign flag; most recent operation yielded a negative value
              OF - overflow flag; most recent operation yielded two's complement overflow

          Instructions to set condition codes
              ---------------------------------------------------------
              Instruction     Based on      Types
              ---------------------------------------------------------
              cmp S1, S2      S2 - S1       cmpb, cmpw, cmpl, cmpq
              test S1, S2     S1 & S2       testb, testw, testl, testq
              ---------------------------------------------------------
              * similar to 'sub' and 'and' respectively, except they don't alter dest loc
              * arithmetic and logical instructions also set condition codes

          There are 3 common ways of using condition codes:
              - set a low-order byte to 0 or 1 (and clear high-order bytes for 32-bit/64-bit result)
                  The set instructions
                  --------------------------------------------------------------------------------------
                  Instruction     Synonym         Effect                  Set condition
                  --------------------------------------------------------------------------------------
                  sete  D         setz            D ← ZF                  Equal / zero
                  setne D         setnz           D ← ~ ZF                Not equal / not zero
                  sets  D                         D← SF                   Negative
                  setns D                         D← ~ SF                 Nonnegative
                  setg  D         setnle          D ← ~ (SF ^ OF) & ~ZF   Greater (signed >)
                  setge D         setnl           D ← ~ (SF ^ OF)         Greater or equal (signed >=)
                  setl  D         setnge          D ← SF ^ OF             Less (signed <)
                  setle D         setng           D ← (SF ^ OF) | ZF      Less or equal (signed <=)
                  seta  D         setnbe          D ← ~ CF & ~ZF          Above (unsigned >)
                  setae D         setnb           D ← ~ CF                Above or equal (unsigned >=)
                  setb  D         setnae          D ← CF                  Below (unsigned <)
                  setbe D         setna           D ← CF | ZF             Below or equal (unsigned <=)
                  --------------------------------------------------------------------------------------
                  * suffixes denote different conditions (not different operand sizes)

              - conditionally jump to some other part of the program
                  The jump instructions
                  -----------------------------------------------------------------------------------------
                  Instruction         Synonym       Jump condition            Description
                  -----------------------------------------------------------------------------------------
                  jmp Label                         1                         Direct jump
                  jmp *Operand                      1                         Indirect jump
                  je  Label           jz            ZF                        Equal / zero
                  jne Label           jnz           ~ZF                       Not equal / not zero
                  js  Label                         SF                        Negative
                  jns Label                         ~SF                       Nonnegative
                  jg  Label           jnle          ~(SF ^ OF) & ~ZF          Greater (signed >)
                  jge Label           jnl           ~(SF ^ OF)                Greater or equal (signed >=)
                  jl  Label           jnge          SF ^ OF                   Less (signed <)
                  jle Label           jng           (SF ^ OF) | ZF            Less or equal (signed <=)
                  ja  Label           jnbe          ~CF & ~ZF                 Above (unsigned >)
                  jae Label           jnb           ~CF                       Above or equal (unsigned >=)
                  jb  Label           jnae          CF                        Below (unsigned <)
                  jbe Label           jna           CF | ZF                   Below or equal (unsigned <=)
                  -----------------------------------------------------------------------------------------
                  * jump instruction target encodings
                      - (addr of target instr) - (addr of instr immediately following the jump)
                      - abs addr of target instr 

              - conditionally transfer data
                  The conditional move instructions
                  --------------------------------------------------------------------------------------
                  Instruction       Synonym         Move condition        Description
                  --------------------------------------------------------------------------------------
                  cmove  S, R       cmovz           ZF                    Equal / zero
                  cmovne S, R       cmovnz          ~ZF                   Not equal / not zero
                  cmovs  S, R                       SF                    Negative
                  cmovns S, R                       ~SF                   Nonnegative
                  cmovg  S, R       cmovnle         ~(SF ^ OF) & ~ZF      Greater (signed >)
                  cmovge S, R       cmovnl          ~(SF ^ OF)            Greater or equal (signed >=)
                  cmovl  S, R       cmovnge         SF ^ OF               Less (signed <)
                  cmovle S, R       cmovng          (SF ^ OF) | ZF        Less or equal (signed <=)
                  cmova  S, R       cmovnbe         ~CF & ~ZF             Above (unsigned >)
                  cmovae S, R       cmovnb          ~CF                   Above or equal (Unsigned >=)
                  cmovb  S, R       cmovnae         CF                    Below (unsigned <)
                  cmovbe S, R       cmovna          CF | ZF               Below or equal (unsigned <=)
                  --------------------------------------------------------------------------------------
                  * Can only be used when branching expressions don't generate errors or have side effects. 
                  These cases are fairly common and provide a much better match to the operation of modern processors

          Loops 
              - do while 
                  jump-to-middle except initial test is omitted

              - while 
                  ------------------------
                  1. jump to middle
                  ------------------------
                    jump test
                  loop:
                    execute body
                  test:
                    test
                    if (test) jump loop
                  ------------------------

                  ------------------------
                  2. guarded do (-O1 flag in GCC)
                  ------------------------
                    test
                    if (test) jump done
                  loop:
                    execute body
                    test
                    if (test) jump loop
                  done:
                  ------------------------

              - for 
                  either jump-to-middle or guarded-do depending on compiler optimization
          Switch statement
              - jump table (each entry is addr of corresponding switch index (i) code segment) & switch index

  c) Implementation of procedures

      A program can manage the storage required by its procedures (functions) using a stack,
      where the stack and the program registers store the information required for
      passing control and data, and for allocating memory

          Run-time stack
                      stack 'bottom'
              ----------------------------      
              |             .            |
              |             .            |      earlier frames
              |             .            |
              ---------------------------- 
              |             .            |      |
              |         argument n       |      |
              |             .            |      | frame for calling
              |             .            |      | function P
              |         argument 7       |      |
              | ------------------------ |      |      
              |       return address     |      |      
              ----------------------------       
              |      saved registers*    |      |
              |      local variables     |      | frame executing  
              ----------------------------      | function Q (called by P)
              |   argument build area    |      |
              ----------------------------        %rsp (stack pointer)
                      stack 'top'
              * %rbx, %rbp and %r12-%r15 Q has to preserve content of these registers; if it wants those registers, 
              it has to push content of those registers to 'saved registers' area of stack, use them and restore
              them back from the stack 

          a) Passing control
              --------------------------------
              Instruction     Description
              --------------------------------
              call Label      Procedure call
              call *Operand   Procedure call
              ret             Return from call
              --------------------------------
              * 'call label' pushes the next instr's addr to stack; then jumps to 'label' addr
              * 'ret' pops the next instr's addr from the stack and jumps to it

          b) Passing data
              * the first six args are passed in registers (%rdi, %rsi, %rdx, %rcx, %r8, %r9)
              * rest in the stack

          b) Allocating and deallocating memory

  d) Implementation of data structures (arrays, structures & unions).
      Pointers
          Key pointer principles
              Every pointer has an associated type
              Every pointer has a value
              Pointers are created with the ‘&’ operator
              Pointers are dereferenced with the ‘*’ operator
              Arrays and pointers are closely related
              Casting from one type of pointer to another changes its type but not its value
              Pointers can also point to functions

          Some expressions involding an array E
          ----------------------------------------------------------------------
          Expression    Type      Value           Assembly code
          ----------------------------------------------------------------------
          E             int *     xE              movl %rdx,%rax
          E[0]          int       M[xE]           movl (%rdx),%eax
          E[i]          int       M[xE+ 4i]       movl (%rdx,%rcx,4),%eax
          &E[2]         int *     xE + 8          leaq 8(%rdx),%rax
          E+i-1         int *     xE + 4i − 4     leaq -4(%rdx,%rcx,4),%rax
          *(E+i-3)      int       M[xE + 4i − 12] movl -12(%rdx,%rcx,4),%eax
          &E[i]-E       long      i               movq %rcx,%rax
          ----------------------------------------------------------------------
          * E (array of int) and i (index) are stored in %rdx and %rcx, respectively
          * result stored in either %eax (for data) and %rax (for pointers)
          
      * C compiler is able to make many optimizations for code operating on multidimensional arrays of fixed size

      Structs/Unions
          - Data Alignment
              Many computer systems place restrictions on the allowable addresses for the
              primitive data types, requiring that the address for some objects must be a multiple
              of some value K (typically 2, 4, or 8).

              The x86-64 hardware will work correctly regardless of the alignment of data.
              However, Intel recommends that data be aligned to improve memory system
              performance. Their alignment rule is based on the principle that any primitive
              object of K bytes must have an address that is a multiple of K.
                  ---------------------------
                  K     Typeskkkk
                  ---------------------------
                  1     char
                  2     short
                  4     int, float
                  8     long, double, char *
                  ---------------------------

  e) Out-of-bounds memory references and buffer overflow attacks.
  
      * Stack randomization (a class of address-space layout randomization [ASLR])

      * Stack corruption detection
          Inserts a canary value in the stack frame between local buffet and rest of stack state. 
          Before restoring the register state and returning from the function, check this canary 
          value; if it is altered, abort with an error.  

      * Limiting executable code regions

  f) Floating-point code

----------------------------------------------------------
4. Processor Architecture 
----------------------------------------------------------

a) Y86-64 (simple instruciton set)
    * Components of its state
        - 15 program registers (all x86-64 registers except %r15)
            * each can hold 64-bit values
            * %rsp is a stack pointer (for push/pop/call/return)
        - three single-bit condition codes (ZF, SF, OF)
        - program counter (PC)
        - status code (Stat) for error indication

    * Set instructions and their encodings
        -----------------------------------------------------------------------------
        Byte              0     1       2     3     4     5     6     7     8     9
        -----------------------------------------------------------------------------
        halt              0 0 
        nop               1 0 
        rrmovq rA, rB     2 0   rA rB   
        irmovq V, rB      3 0   F  rB   ----------------------- V -------------------
        rmmovq rA, D(rB)  4 0   rA rB   ----------------------- D -------------------
        mrmovq D(rB), rA  5 0   rA rB   ----------------------- D -------------------
        OPq rA, rB        6 fn  rA rB   
        jXX Dest          7 fn  ---------------------------- Dest ---------------
        cmovXX rA, rB     2 fn  rA rB   
        call Dest         8 0   ---------------------------- Dest ---------------
        ret               9 0 
        pushq rA          A 0   rA F   
        popq rA           B 0   rA F   
        -----------------------------------------------------------------------------
        * instr encodings range between 1-10 bytes
            - 1-byte instr specifier
            - possibly 1-byte register specifier
            - possibly 8-byte constant word 
        * fn specifies particular int operation, data movement condition, or branch condition
        * four types of move instr (regis-to-regis (rr), immed-to-regis (ir), memory-to-register (mr), etc)
        * OPq     = addq, subq, andq, xorq (fn ranges from 0 to 3)
        * jXX     = jmp, jle, jl, je, jne, jge, jg (jump instrs) (fn ranges from 0 to 6)
        * cmovXX  = cmovle, cmovl, cmove, cmovne, cmovge, cmovg (conditional move instrs) (fn ranges from 0 to 6)
        * 8-byte const word = 'immed data' for irmovq, 'displacement' for rmmovq and mrmovq address specifiers and 'dest' of branches and calls  
        
        *** [rmmovq %rsp, 0x123456789abcd (%rdx)] encoding in hex is 4042cdab896745230100
            - 40 is rmmovq instr code
            - 42 is src/dest register codes
            - cdab896745230100 is padded word for displacement
    
    * Programming conventions
    * Handling of exceptional events
        Status code 
        ------------------------------------------------
        Value   Name    Meaning
        ------------------------------------------------
        1       AOK     Normal operation
        2       HLT     halt instruction encountered
        3       ADR     Invalid address encountered
        4       INS     Invalid instruction encountered
        ------------------------------------------------

b) Background on digital hardware design.

    * Basic buildign blocks of a processor (or any digital system)

        - combinational logic   
            * combinational logic circuits (a number of logic gates wired together) perform different types of operations on word-level data

        - memory elements       
            * clocked registers (for PC, CC and Stat)
            * random access memory (RAM)
                - virtual memory (instruction/data memory)
                - register file

        - clock signals         
            * to regulate the updating of the memory elements

    * HCL language

c) Y86-64 processor
    * Sequential Y86-64 processor
        - fetch         - read bytes of an instruction from memory using PC
        - decode        - read up to two operands from register file
        - execute       - perform arith operation | compute effective address of a mem ref | incr/decr stack pointer
        - memory        - read/write from/to memory
        - write back    - write up to two results to register file
        - PC update     - set PC to address of next instruction

    * Pipelined Y86-64 proessor

        - limitations of pipelining
            1) nonuniform partitioning
            2) diminishing returns of deep pipelining

        - Changes needed to make the sequencial processor pipelined

            * PC update, fetch, decode, execute, memory, write back
            * Insert pipeline register between each stage
            * Implement Pipeline Control Logic

                - handle branch prediction (bc a throughput is around one instr per cycle and smt you don't know which instr to process next)
                - handle pipeline hazards (dependencies causing erroneous computation)

                    * data hazards
                        - stalling (holding back an instr in the decode stage until the instrs generating its 
                        source operands have passed through the write-back stage)
                        - forwarding (passing a result value directly from one pipeline stage to an earlier one)
                    * control hazards
                        - stalling

                - handle processing ret
                - handle exceptions
                    * add hardware to carry exception status code along with other info about instr through the pipeline
                    * in memory or write-back stage, detect the exception and disallow changing the state by 
                    following instrs in execute stage

            * Add more complex features

                - handle multicycle instrs (int arith and floating-point operations)
                    * [option 1] expand capabilities of execute stage; instr remains in this stage for as many clock cycles as it 
                    requires, causing fetch and decode stages to stall [very slow]
                    * [option 2] add special hardware functional units to handle such complex operations

                - handle interfacing with memory system 
                    * use stalling for short-duration cache misses
                    * use exception handling for lon-duration page faults (getting data from disk)

d) Tools
    * Assembler for Y86-64
    * Simulator for running Y86-64 programs
    * Simulator for two sequencial and one pipelined processor design

----------------------------------------------------------
5. Optimizing Program Performance 
----------------------------------------------------------
1. Use efficient algorithms and data structures
    a) Eliminate unnecessary work
        - loop inefficiencies (functional calls, conditional tests, memory references)
            * code motion - identify computations performed multiple times but
              whose result won't change.

            * loop unrolling - reduce the # of iterations by computing more
              elements on each iteration. (k x 1 loop unrolling)
              ----------------------------      -------------------------------------------
              | for i in range(0, N):    |  ->  | for i in range(0, N, 2):                |
              |     sum = sum + array[i] |  ->  |     sum = sum + array[i] + array[i + 1] |    
              ----------------------------      -------------------------------------------

            * to reduce memory references, introduce local variables accumulating within loops

2. Write code that compilers can optimize
    a) Understand capabilities and limitations of optimizing compilers
        - know optimization blockers
            * memory aliasing - If a compiler cannot determine whether or not
              two pointers may designate the same location, it must assume that 
              either case is possible, limiting the set of possible optimizations.
            [fix] define local variables to tell compiler not to check for aliasing

            * function calls - most compilers don't try to determine if a
              function is free of side effects, hence they leave those calls
              intact, limiting the set of possible optimizations.
            [fix] use inline functions (GCC with -O1 within single file)

3. Use parallelism capabilities of modern processors
    a) Microarchitecture of processor
        - instruction level parallelism (ILP)

        - latency bound - a series of operations must be performed in strict
          sequence, limiting the processor to use ILP. 

        - throughput bound - raw computing capacity of the processor's
          functional units, ultimate limit on program performance.
        
        - Modern processors
            * Instruction control unit (ICU)
                a) fetch control
                b) instruction cache
                c) instruction decoder

            * Execution unit (EU)
                a) functional units (store, load, arithmetic, branch)
                    - latency - total time required to perform operation.

                    - issue time - min # of clock cycles between 2 independent
                      operations of the same type (# of operations that can be
                      started on each clock cycle)

                    - capacity - # of functional units capable of performing
                      that operation.

                    * a processor can potentially achieve a throughput of C/I
                      operations per clock cycle (C - capacity, I - issue time)

                b) data cache

        - Enhancing parallelism - breaking latency bound
            * multiple accumulators (k x k loop unrolling)
              for associative and commutative operations (int +|*), split the 
              set of combining operations into X parts and combine them at the end. 
              That way, the processor doesn't need to delay one operation until 
              the prev one finishes. In such a case, instead of one critical-path, you have multiple, 
              keeping the pipelines filled for all functional units capable of performing that operation. 
              ----------------------------      -------------------------------------------
              | for i in range(0, N):    |      | for i in range(0, N, 3):                |
              |     sum = sum + array[i] |  ->  |     sum1 = sum1 + array[i]              |    
              ----------------------------      |     sum2 = sum2 + array[i + 1]          |
                                                |     sum3 = sum3 + array[i + 2]          |
                                                | sum = sum1 + sum2 + sum3                |
                                                -------------------------------------------

            * reassociation transformation (k x 1a loop unrolling)
              can reduce the number of operations along the critical path in a computation, 
              resulting in better performance by better utilizing the multiple functional 
              units and their pipelining capabilities. 
              -------------------------------------
              | a + b + c + d = (a + b) + (c + d) |
              -------------------------------------

            ** In general, we have found that unrolling a loop and accumulating
            multiple values in parallel is a more reliable way to achieve
            improved program performance (than reassociation transformation).

        - Some limiting factors
            * Regiter Spilling - if a program has a degree of parallelism that
              exceeds # of available registers, then the compiler will resort
              to 'spilling', storing some of the temp values in memory.

            * Branch prediction and misprediction penalties
                - write code that favors conditional data transfers over
                  control transfers.

        - Understanding Memory Performance
            * Load and store 

4. Issues related to optimizing large programs in the real world
    a) High-level design
        - Avoid algos and ds with asymptotically poor performance
    b) Basic coding principles
        - Eliminate excessive func calls
            * move computations out of the loop if possible
            * trade off modularity to gain greater efficiency
        - Eliminate excessive memory refs
            * introduce temp variables to hold intermediate results
            * store result in an array or global variable only when
              finished
    c) Low-level optimizations
        - Unroll loops to reduce overhead and to enable further
          optimizations
        - Increase instruction-level parallelism by using techniques such
          as multiple accumulators and reassociation
        - Rewrite conditional operations in a functional style to enable
          compilation via conditional data transfers

A good strategy: 
  - experiment and analyze
  - look at inner loops (excessive memory references & poor use of registers)
  - identify critical paths, which gives you a lower bound on time and try to
    steer the compiler towards more efficient implementations, which means
    doing a lot of rewriting of a program

----------------------------------------------------------
6. The Memory Hierarchy 
----------------------------------------------------------
1. Storage technologies

    a) RAMs (random access memories) - volatile
        - SRAM (static)   - faster/more expensive; used for cache.
        - DRAM (dynamic)  - slower/less expensive; used for main memory and graphics frame buffers.
            * data flows back and forth between CPU and DRAM memory over buses
            * bus - a collection of parallel wires that carry addr, data and control signals

    b) ROMs (read-only memories) and flash memories - nonvolatile
        - since ROMs retain information even after they are turned off (= nonvolatile), certain kind of programs 
        are stored in them to run when the system is powered up (e.g., PC's BIOS routines) 

    c) Disks 
        - Types
            * Rotating
            
                - Spingle, platters (2 surfaces on each), tracks, sectors (each 512 bytes) & gaps 
                    * each surface consists of a collection of concentric rings called tracks
                    * each track is partitioned into a collection of sectors, each of which contains equal # of data bits
                    * sectors are separeted by gaps, which are formatting bits that id sectors

                - Cylinder - the collection of tracks on all the surfaces that are equidistant from the center of the spindle

                - Recording Zone
                    * the set of cylinders is partitioned into disjoint subsets known as recording zones
                    * each zone consists of a contiguous collections of cylinders
                    * each track in each cylinder in a zone has the same # of
                      sectors, which is determined by the # of sectors that can be
                      packed into the innermost track of the zone.
                      
                - Logical Disk Blocks (0, 1, ... B - 1) - simple view of disk geometry presented to the OS
                    * disk controller stands in-between OS and disk,
                      translating 'logical block' addresses used by the OS to
                      (surface, track, sector) triple that ids particular sector. Then
                      hardware finds this sector, reads bits, and copies them to main memory

            * SSDs (solid state) based on flash memory
                - A sequence of blocks (each is 32-128 pages), pages (each is 512B to 4KB)
                - Data are read/written in units of pages; a page can be written only after 
                  the entire block to which it belongs to has been erased. 
                - Flash translation layer stands in-between the OS and the flash memory 

2. Locality
    a) temporal - same location is likely to be accessed multiple times in the future

    b) spacial - nearby locations are likely to be accessed in the future

    c) Summary
        * functions that repeatedly reference the same variables enjoy good temporal locality 
        * for programs with stride-k reference patterns, the smaller the stride, the better spacial locality;
          stride-1 reference patterns are a common and important src of spacial locality.
        * loops have good temporal and spacial locality with respect to instruciton fetches. The smaller 
          the loop body and the greater # of loop iterations, the better the locality. 

3. Memory Hierarchy (L0 <= L1 <= L2 <= L3 <= L4 <= L5 <= L6)

                    --- L0 ---                | CPU registers
                  ----- L1 -----              | SRAM
                ------- L2 -------            | SRAM
              --------- L3 ---------          | SRAM
            ----------- L4 -----------        | DRAM
          ------------- L5 -------------      | local disks
        --------------- L6 ---------------    | distributed file systems, web servers

        ------------------------------------------------------------------------------------------------------------
        Type              What cached               Where cached            Latency (cycles)    Managed by
        ------------------------------------------------------------------------------------------------------------
        CPU registers     4-byte or 8-byte words    On-chip CPU registers   0                   Compiler
        TLB               Address translations      On-chip TLB             0                   Hardware MMU
        L1 cache          64-byte blocks            On-chip L1 cache        4                   Hardware
        L2 cache          64-byte blocks            On-chip L2 cache        10                  Hardware
        L3 cache          64-byte blocks            On-chip L3 cache        50                  Hardware
        Virtual memory    4-KB pages                Main memory             200                 Hardware + OS
        Buffer cache      Parts of files            Main memory             200                 OS
        Disk cache        Disk sectors              Disk controller         100,000             Controller firmware
        Network cache     Parts of files            Local disk              10,000,000          NFS client
        Browser cache     Web pages                 Local disk              10,000,000          Web browser
        Web cache         Web pages                 Remote server disks     1,000,000,000       Web proxy server
        ------------------------------------------------------------------------------------------------------------

    a) Cache hits/missses
        - Types of misses
            * conflict miss 
            * capacity miss
            
    b) Organization
        - Organization
            * cache is an array of sets
            * each set contains one or more lines
            * each line contains a valid bit, [m - (b + s)] tag bits and [2^b] block of data
            * cache organization induces a partition of the m address bits into t tag bits, s set index bits, and b block offset bits
                - s ids a set in an array
                - t ids a line in the found set
                    * valid bit must be set
                - b ids offset of the word in the B-byte data block of the line 

            * cache organization can be characterized by the tuple (S, E, B, m)
            * m - # of bits that form M = 2^m unique addresses in a system
            * S = 2^s sets; E = lines in a set; B = 2^b bytes in a line

        - Cache address
            ---------------------------------------------------
            |   tag bits  |   set bits  |   block-offset bits |
            ---------------------------------------------------
            m - 1                                             0
            * set bits correspond to unsigned set number (0 to S-1)
            * tag bits identify the line in the set
            * block-offset leads us to the start bit of data we are interested in

    c) Types
        - Direct-mapped cache       - one-line-per-set cache 
        - E-way associative cache   - E-lines-per-set cache
        - Fully associative cache   - a single set containing all lines 
        - i-cache (read-only)       - can only hold instructions
        - d-cache                   - can only hold data
        - unified-cache             - can hold both instructions and data

    d) Performance
        - miss rate       = #misses/#references
        - hit rate        = 1 - misses
        - hit time        = set selection + line identification + word selection
        - miss penalty    = additional time required because of a miss
            ------------------------------------------------------
            cache       penalty (clock cycles)
            ------------------------------------------------------
            L1          10  (L1 has to get data from L2)
            L2          50  (L2 has to get data from L3)
            L3          200 (L3 has to get data from main memory)
            ------------------------------------------------------
    
    e) Conclusion
        The performance of the memory system is a mountain of temporal and
        spatial locality whose elevations can vary by over an order of
        magnitude.Wise programmers try to structure their programs so that they
        run in the peaks instead of the valleys. The aim is to exploit temporal
        locality so that heavily used words are fetched from the L1 cache, and
        to exploit spatial locality so that as many words as possible are
        accessed from a single L1 cache line.

        - Focus your attention on the inner loops, where the bulk of the computations
          and memory accesses occur.
        - Try to maximize the spatial locality in your programs by reading data objects
          sequentially, with stride 1, in the order they are stored in memory.
        - Try to maximize the temporal locality in your programs by using a data object
          as often as possible once it has been read from memory

----------------------------------------------------------
7. Linking 
----------------------------------------------------------
Linkers allow us to decompose large programs into smaller, more manageable
modules that can be modified and compiled/linked separately.

1. Preprocessor (.i) => compiler (.s) => assembler (.o) => linker (progam) => loader => CPU/memory

2. Types
    a) Static linking
        * at compile time
        * relocatable obj files (including static libraries) => executable obj file
        * static libraries - a copy goes to each program that references it

    b) Dynamic linking
        * at load or run time
        * shared obj files + partially linked exec obj files in memory => executable obj file
        * shared libraries [.so in Linux | ddl in MS] - one copy is shared by all programs

2. Obj files
    a) Types
        - relocatable
            * Relocatable obj file format
                -------------------------
                |     ELF header        |   - metadata about this file
                -------------------------
                |       .text           |   - machine code of the compiled program
                |       .rodata         |   - read-only data (e.g. format strs & jump tables)
                |       .data           |   - initialized global and static C variables [1] 
                |       .bss            |   - uninitialized global and static variables (incl. ones that are equal 0)
                |       .symtab         |   - symbol table with info about funcs and global variables defined/ref'd in the program
                |       .rel.text       |   - a list of locs in .text section that need to be modified 
                |       .rel.data       |   - see above
                |       .debug          |   - debug symbol table
                |       .line           |   - mapping between C source program and machine instruction in .text
                |       .strtab         |   - string table for .symtab and .debug and for section names in section headers
                -------------------------
                | Section haeder table  |   - locs and sizes of various sections
                -------------------------
                [1] - local nonstatic variables are managed at run time on the stack
                * linker doesn't care about local variables

            * Symbol resolution
                - Duplicates in regular files
                    - Rule 1. Multiple strong symbols with the same name are not allowed.
                    - Rule 2. Given a strong symbol and multiple weak symbols with the same name, choose the strong symbol.
                    - Rule 3. Given multiple weak symbols with the same name, choose any of the weak symbols.
                      * initialized = strong; uninitialized = weak
                      * rules 2 & 3 are a source of nasty bugs

                -  Resolution with static libraries (a package of related obj modules in a single file)
                    * Process
                        - Include the header file (.h) of the shared library in your
                          file (.h => symbols)
                        - Pass shared library archive file (.a) to the linker with
                          your own relocatable obj files (.a => definitions of
                          the above symbols) [1]

                        [1] the linker scans input files in the same order they
                          appear in the compiler driver's command line (ordering of inputs matters). 
                          Hence if the definition (e.g., in static library archive
                          file) comes before the reference (e.g., in user file that used
                          the library) and not after, the linker cannot resolve
                          that symbol and will fail. So, the general rule with
                          libraries is to place them at the end of command line.

            * Relocation
                - Relocating sections and symbol definitions
                - Relocating symbol references within sections
                    * relocation entries (when the assempler encounters
                      references to objects whose ultimate locs are unknown, it
                      generates relocation entries for those references)
  
        - executable
            * Executable obj file format
                -------------------------
                |     ELF header        |   - metadata about this file
                -------------------------
                |  Segment header table |   - maps contiguous file sections to run-time memory segments
                -------------------------
                |       .init           |   - defines _init, which is called by the program init code
                |       .text           |   - machine code of the compiled program
                |       .rodata         |   - read-only data (e.g. format strs & jump tables)
                |       .data           |   - initialized global and static C variables [1] 
                |       .bss            |   - uninitialized global and static variables (incl. ones that are equal 0)
                |       .symtab         |   -- 
                |       .debug          |    | not loaded into memory
                |       .line           |    |
                |       .strtab         |   -- 
                -------------------------
                | Section haeder table  |   - locs and sizes of various sections
                -------------------------

            * Linux x86-64 run-time memory image (when the above loads into memory)

                ----------------------------      2^(48)-1
                |       User stack         |
                |   (created at run time)  |
                ---------------------------- %esp (stack pointer)
                |             ↓            |
                |             ↑            |
                ----------------------------
                | Memory-mapped region for |
                |     shared libraries     |
                ----------------------------
                |                          |
                |             ↑            |
                ---------------------------- brk
                |      Run-time heap       |
                |   (created by malloc)    |
                ---------------------------- --
                |   Read/write segment     |  |
                |      (.data,.bss)        |  |
                ----------------------------  | loaded from executable file
                |   Read-only code segment |  |
                |   (.init,.text,.rodata)  |  |
                ---------------------------- --   0x400000
                |                          |
                ----------------------------      0

        - shared
            * shared libraries - modern sofware innovation that address the disadvantages of
              static libraries (for each running program, you must have separate
              copy, which is wasteful of memory space)
            * loaded at an arbitrary memory address and linked with a prog in memory
            * only one copy of the library across one file system 
            * linking is done in two phases:
                - regular linking except no actual copying (from library to
                  to-be-created final executable obj file) takes place
                - partially linked obj file is then loaded into memory and is
                  dynamically linked at run-time

----------------------------------------------------------
8. Exceptional Control Flow 
----------------------------------------------------------

    E table, E number, kernel mode (vs user mode), E handler
    * E - exception
    
    a) Exceptions (hardware interrupt => sofware handler => effect)

        -----------------------------------------------------------------------------------------------
        Class       Cause                             Async/sync      Return behavior
        -----------------------------------------------------------------------------------------------
        Interrupt   Signal from I/O device            Async           Next instruction
        Trap        Intentional exception             Sync            Next instruction
        Fault       Potentially recoverable error     Sync            Current instruction | Abort
        Abort       Nonrecoverable error              Sync            Abort
        -----------------------------------------------------------------------------------------------

        Interrupt (I/O signals): network adapters, disk controllers, etc
        Trap (e.g., system call via syscall): read, fork, execve, exit
        Fault (recoverable errors): page fault exception
        Aborts (fatal errors): SRAM/DRAM bits are corrupted

    b) Process (kernel (OS) => process abstraction => applications)
          - Main abstractions
              * independent logical control flow abstraction
              * private address space abstraction
          - Context switch 
              * built on top of the lower-level ECF a)

    c) Signals (Applications => signals => OS => effects)
        - Higher-level software form of exceptional control flow that allows
          processes and the kernel to inturrupt other processes.

        - At any point in time, a process can have one of type k pending signal
          (sent but not received yet). Subsequent k-type signals are discarded. 

        - Transfer of control to a destination process occurs in two steps
            * Sending a signal: process group, /bin/kill, kill, alarm
            
            * Receiving a signal
                - The process terminates.
                - The process terminates and dumps core.
                - The process stops (suspends) until restarted by a SIGCONT signal.
                - The process ignores the signal.
                  * A process can modify the default action associated with a
                    signal by using the 'signal' function.
                  * Signal handlers can be interrupted by other handlers (nested handling)

            * Blocking/unblocking signals

        - Writing signal handlers
            * Keep handlers as simple as possible
            * Call only async-signal-safe functions in your handlers (don't allow nested handling?)
            * Save and restore errno (leave errno unchanged since other parts of the program might use it)
            * Protect accesses to shared global data structures by blocking all signals
            * Declare global variables with volatile (tells compiler not to cache the variable)
            * Declare flags with sig_atomic_t

    d) Nonlocal jumps (Application => bypass call/return discipline)
        - User-level exceptional control flow that transfers control directly
          from one func to another currently executing func and is provided by
          the setjmp and longjmp functions
        - Applications
            * permit an immediate return from a deeply nested function call
            * branch out of a signal handler to a specific code location,
              rather than returning to the instruction that was interrupted by
              the arrival of the signal

----------------------------------------------------------
9. Virtual Memory 
----------------------------------------------------------

  a) Virtual memory (VM) is an abstraction of main memory that provides each
  process with a large, uniform, and private address space.

      - It uses main memory efficiently by treating it as a cache for an address space
      stored on disk, keeping only the active areas in main memory and transferring
      data back and forth between disk and memory as needed. 

          * Disk pages <=> virtual pages <=> memory pages

              - Virtual pages 
                  ----------------------------------------------------------------------------------------
                  Type                                              Page Table Entry          CPU read
                  ----------------------------------------------------------------------------------------
                  - Unallocated (no data associated with the page)  [0][null]
                  - Cached (page data is in main memory)            [1][address in memory]    Page hits
                  - Uncached (page data is in disk)                 [0][address in disk]      Page faults
                  ----------------------------------------------------------------------------------------
                  * swapping/paging - transfering of a page between disk/memory
                  * demand paging - waiting until last moment to do the transfer
                  * thrashing - swapping of pages in/out continuously

      - It simplifies memory management by providing each process with a uniform address space. 

          * multiple virtual pages (diff apps instructions) can be mapped to
            the same physical page (common lib instructions the apps use)

          * demand paging & unique (tho same format) virtual address space for each process
              - simplifies linking, loading, sharing, memory allocation 

      - It protects the address space of each process from corruption by other processes.

          * by adding some bits (mode, read, write) to the page table entry, VM
            can provide page-level memory protection

  b) This chapter looks at virtual memory from two angles. 

      - The first half of the chapter describes how virtual memory works. 

      - The second half describes how virtual memory is used and managed by applications.

  c) Address translation
      - CPU => VA => MMU => PTE address => main memory => PTE => MMU => PA =>
        main memory => requested data // hit

      ------------------------------------------------------------
      Symbol       Description
      ------------------------------------------------------------
      Basic parameters
      N = 2^n      Number of addresses in virtual address space
      M = 2^m      Number of addresses in physical address space
      P = 2^p      Page size (bytes)

      Components of a virtual address (VA)
                  n-1 p p-1 0
      VA          [VPN] [VPO]
      VAS         N-element Virtual address space
      VPO         Virtual page offset (bytes)
      VPN         Virtual page number
      TLBI        TLB index
      TLBT        TLB tag

      Components of a physical address (PA)
                  m-1 p p-1 0
      PA          [PPN] [PPO]
      PAS         M-element physical address space
      PPO         Physical page offset (bytes)
      PPN         Physical page number
      CO          Byte offset within cache block
      CI          Cache index
      CT          Cache tag

      Components of a page table
      PT          Page table
      PTBR        Page table base register (in CPU)
      PTE         Page table entry
      ------------------------------------------------------------

      PTBR points to the PT; VPN in VA acts as a index to the PT; PTE contains
      PPN, which when combined with VPO in a VA, forms a PA. 

      
            * Linux x86-64 run-time memory image (when the above loads into memory)

          ---   ----------------------------    ----
         |      |     Process-spec data    |        |
         |      |   (e.g., page tables,    |    different for
         |      |    task and mm structs,  |    each process 
      kernel    |       kernel stack       |        | 
      virtual   ----------------------------    ----      
      memory    |     Physical memory      |        |
         |      ----------------------------    identical for each process
         |      |   Kernel code and data   |        |
          ---   ----------------------------    ----      
                |       User stack         |
                |   (created at run time)  |
                ---------------------------- %esp (stack pointer)
                |             ↓            |
                |             ↑            |
                ----------------------------
                | Memory-mapped region for |
                |     shared libraries     |
                ----------------------------
                |                          |
                |             ↑            |
                ---------------------------- brk
                |      Run-time heap       |
                |   (created by malloc)    |
                ----------------------------   ----
                |   Read/write segment     |       |
                |      (.data,.bss)        |       |
                ----------------------------   loaded from executable file
                |   Read-only code segment |       |
                |   (.init,.text,.rodata)  |       |
                ----------------------------   ----   0x400000
                |                          |
                ----------------------------          0

  d) Memory mapping
      - Manual allocator (mmap)
      - Dynamic allocator (malloc)
          * malloc manages memory in the heap (an area of virtual address space)

      - Explicit allocator
      - Implicit allocator (garbage collectors)

  e) Common errors
      - dereferencing bad pointers
      - reading uninitialized memory
      - allowing stack buffer overflows 
      - assuming that pointers and the objects they point to are the same size
      - referencing a pointer instead of the object it points to
      - misunderstanding pointer arithmetic
      - referencing nonexistent variables
      - introducing memory leaks

----------------------------------------------------------
10. System-Level I/O
----------------------------------------------------------

I/O is the process of copying data between main memory and external devices.

Read/write are subject to short counts

Unix I/O
  a) All I/O devices are modeled as files and all input/output is performed by
  reading/writing the appropriate files.

  b) File
      - Type
          * Regular - contains arbitrary data
          * Directory - contains an array of links to other files, . (itself) and .. (its parent)
          * Socket - a file used to communicate with other processes
          * Named pipes, Symbolic links, Character, Block

      - Opening files
          * descriptor (tracked by the application), open file (tracked by the kernel)
          * each process begins life with three open files (stdinput, stdoutput, standerr)

      - Changing the current file position
          * kernel maintains 'file position k' (init to zero), a byte offset from the beginning of a file

      - Reading/writing files
          * Read 
              - copy n bytes from a file to memory, starting at k until n + k
              - increment k by n
              - given a file size m, if k >= m, trigger 'end-of-file' (EOF)
          * Write
              - copy n bytes from memory to a file, startign at k and then updating k
          * Sometimes, read/write transfer fewer bytes than the application
            requests. Such 'short counts' don't indicate an error. 

      - Closing files
          * Kernel frees data structures related to open files and adds descriptor to the available pool

      - DS of open files and their role in file sharing and I/O redirection 
          * Representing open files 
              - descriptor table - unique for each process; indexed with open
                file descriptors; each entry points to an entry in file table

              - file table - shared by all processes to represent a set of
                open files; each entry consists of a) file position b) a
                reference count of descriptor entries that point to it c)
                pointer to an entry in v-node table

              - v-node table - shared by all processes; each entry contains
                metadata pertaining to one file

          * File sharing
              - Normally, each descriptor references a separate file. In file
                sharing, multiple descriptors reference the same file (same v-node
                table entry) through different file table entries. Unlike this, a parent
                shares files with its children through the same file table entries. 

          * I/O Redirection
              - <|> allows users to associate stdinput/stdoutput with disk files

      - Standard I/O
          * models an open file as a stream, which acts as a pointer to a
            structure of type FILE. A stream of type FILE is an abstraction for
            a file descriptor and a stream buffer, whose purpose is to minimize
            the # of expensive calls to Linux I/O system calls (e.g., instead
            of reading individual bytes from disk in every system call, load
            a block of bytes into a buffer in the 1st call and in subsequent
            calls, serve the function directly from the stream buffer as long
            as there are unread bytes in it).

      - Which I/O functions should I use?
          * Use the standard I/O functions whenever possible
          * Don’t use scanf or rio_readlineb to read binary files
          * Use the Rio functions for I/O on network sockets
            
----------------------------------------------------------
11. Network Programming 
----------------------------------------------------------
a) Network
    - LAN (e.g., Ethernet) - spans a building
        * Ethernet segment - [a hub (small box) - port -- wire -- adapter - hosts]
            - hosts in a segment communicate with each other through a chunk of
              bits called 'frame'
    - Bridged Ethernet - a collection of LANs - spans entire buildings
    - internet - a collection of incompatible LANs/WANs connected through routers
      (specialized computers)
        * WAN (example of high-speed point-to-point phone connections)

        * Hosts in this collection of incompatible LANs/WANs are able
          communicate with each other due to a layer of protocol software
          running on each host and router that smoothes out the differences
              - This protocol software must provide two capabilities
                  1. Naming scheme - assigning addresses to hosts 
                  2. Delivery mechanism - packet - header (size, src/dest addresses) and payload 
          
b) Global IP Internet
    - Global IP Internet 
        Sockets         -------------------                 -------------------                
        interface       | Client          |  User code      | Server          |
        (sys calls) -------------------------               |-----------------|
                        | TCP/IP          |  Kernel code    | TCP/IP          | 
        Hardware    -------------------------               |-----------------|
        interface       | Network adapter |  Hardware       | Network adapter | 
        (interrupts)    -------------------                 -------------------                  
                                        |                    |              
                                        ----------------------
                                        | Global IP Internet |
                                        ----------------------

    - Internet - a collections of hosts
        * each internet host has a unique 32-bit IP address
        * a set of IP addresses is mapped to a set of Internet domain names
        * processes on different Internet hosts can communicate with each other over connections

    - 'Client - socket interface - Server' model
        * Socket is an end of a connection presented to applications as an open
          file with a corresponding descriptor
            - Each socket has an address in the form [address][16-bit int port] 
            - Each connection is identified with (cliaddr:cliport, servaddr:servport)

    - Socket interface
        * Socket address structure [protocol_family][port][IP]

        * Client 
            - creates a socket descriptor with socket constructor
            - calls 'connect' to finish 'opening' the socket
            - if successful, 'clientfd' is now ready for reading/writing

        * Server 
            - calls 'bind' to associate 'sockfd' with server's socket address in addr
            - calls 'listen' to convert 'sockfd' from an active socket into a
              listening one (meaning, turns it into server socket from client
              socket, which is assumed by default by socket constructor)
            - calls 'accept' to wait for connection requests
                * waits for a request to arrive on 'listenfd'
                * fills in the client's socket address in addr
                * returns 'connectedfd' that can be used to
                  communicate with clients using Unix I/O functions

        * Host and Service conversion
            - 'getaddrinfo' turns (hostnames||hostaddresses)(servicename||serviceport)
              into socket address structures (a linked list of (host|service))
            - 'getnameinfo' is the inverse of above
            - these functions' returns used as input to socket constructors 

----------------------------------------------------------
12. Concurrent Programming 
----------------------------------------------------------

a) Three approaches for building concurrent programs
    - Process
        * each logical control flow is a process with separate virtual address space
            - [pro] sharing VAS eliminates a lot of failures 
            - [con] sharing VAS causes inefficiency
            - [con] sharing VAS makes it more difficult to share data

        * scheduled by the kernel
        * flows communicate with each other using interprocess communication (IPC)

    - I/O multiplexing
        * flows, scheduled by applications, are state machines of a single process
        * all flows share the same address space

    - Threads
        * threads - flows, scheduled by the kernel, running in the context of one process 

        * all flows share the same address space but each has its own context

        * shared variables
            - global, local automatic, local static

        * semaphores
            - global variable with a nonnegative integer value that can only be
              manipulated by two special operations (P & V)
            - mutexes are binary semaphores (0|1) 
            - P(s) decrement/return | suspend/V(s)/decrement/return   locking mutex
            - V(s) increment by 1                                     unlocking mutex

            - Uses
                * Mutual exclusion
                    -------------------- 
                      sem_t mutex;              // declare semaphore that protects a variable
                      Sem_init(&mutex, 0, 1);   // initialize mutex to 1

                      P(&mutex);
                      cnt++;                    // protect cnt from other threads
                      V(&mutex);
                    -------------------- 

                * Schedule shared resource
                    - producer-consumer problem
                        * producer -- | buffer | -- consumer
                        * semaphore locks/unblocks the buffer

                    - readers-writers problem
                        * different threads accessing shared resource by either read-only | write access
        * Thread safety

b) Issues
    - Synchronizing concurrent accesses to shared data
        * P and V semaphore operations to 
            - provide mutually exclusive access to shared data
            - schedule access to resources

    - Functions that are called by threads must have a property known as thread safety
        * four classes of thread-unsafe functions
            - does not protect shared variables
            - keeps state across multiple invocations
            - returns pointer to a static variable
            - calls thread-unsafe functions

        * reentrant functions - thread-safe
            - don't access shared data
            - are more efficient bc don't require any synchronization primitives

    - Races and deadlocks
        * Races occur when programmers make incorrect assumptions about how logical flows are scheduled. 
        * Deadlocks occur when a flow is waiting for an event that will never happen

